# Pipelines en Machine Learning

## ¬øQu√© es un Pipeline?

Un **pipeline** (o tuber√≠a) en Machine Learning es una secuencia de pasos de procesamiento de datos y modelado que se ejecutan de forma autom√°tica y ordenada. Es como una cadena de montaje donde los datos pasan por diferentes etapas de transformaci√≥n hasta llegar al modelo final.

### Ventajas de usar Pipelines:

1. **Organizaci√≥n**: Todo el flujo de trabajo est√° en un solo lugar
2. **Reproducibilidad**: Los mismos pasos se aplican siempre en el mismo orden
3. **Prevenci√≥n de fugas de datos**: Evita que informaci√≥n del conjunto de prueba contamine el entrenamiento
4. **Facilidad de mantenimiento**: Cambios centralizados y f√°ciles de gestionar
5. **Despliegue simplificado**: Todo el proceso se puede guardar y reutilizar

---

## Estructura de un Pipeline

```
Datos Crudos ‚Üí Preprocesamiento ‚Üí Transformaci√≥n ‚Üí Modelo ‚Üí Predicci√≥n
```

### Componentes t√≠picos:

1. **Transformadores (Transformers)**: Modifican los datos
   - Imputaci√≥n de valores nulos
   - Escalado de caracter√≠sticas
   - Codificaci√≥n de variables categ√≥ricas
   - Selecci√≥n de caracter√≠sticas

2. **Estimadores (Estimators)**: Aprenden de los datos
   - Modelos de Machine Learning
   - Algoritmos de clasificaci√≥n/regresi√≥n

---

## Ejemplo 1: Pipeline B√°sico con Scikit-Learn

### Dataset: Titanic

```python
import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Cargar datos
df = pd.read_csv('Titanic-Dataset.csv')

# Seleccionar caracter√≠sticas num√©ricas
X = df[['Age', 'Fare', 'SibSp', 'Parch']]
y = df['Survived']

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Crear pipeline
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),  # Paso 1: Imputar valores nulos
    ('scaler', StandardScaler()),                    # Paso 2: Escalar datos
    ('classifier', LogisticRegression())             # Paso 3: Modelo
])

# Entrenar el pipeline completo
pipeline.fit(X_train, y_train)

# Predecir
score = pipeline.score(X_test, y_test)
print(f'Accuracy: {score:.2f}')
```

**Resultado**: El pipeline ejecuta autom√°ticamente:
1. Imputa valores nulos con la mediana
2. Escala las caracter√≠sticas
3. Entrena el modelo de regresi√≥n log√≠stica

---

## Ejemplo 2: Pipeline con ColumnTransformer

### Manejo de variables num√©ricas y categ√≥ricas simult√°neamente

```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier

# Definir caracter√≠sticas num√©ricas y categ√≥ricas
numeric_features = ['Age', 'Fare', 'SibSp', 'Parch']
categorical_features = ['Sex', 'Embarked', 'Pclass']

# Preparar datos
X = df[numeric_features + categorical_features]
y = df['Survived']

# Crear transformadores para cada tipo de variable
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combinar transformadores con ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Pipeline completo
full_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Entrenar
full_pipeline.fit(X_train, y_train)

# Evaluar
score = full_pipeline.score(X_test, y_test)
print(f'Accuracy con Random Forest: {score:.2f}')
```

**¬øQu√© hace este pipeline?**

```
Variables Num√©ricas:
  Age, Fare, SibSp, Parch
        ‚Üì
  Imputar con mediana
        ‚Üì
  Escalar (StandardScaler)
        ‚Üì
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚Üì
Variables Categ√≥ricas: Combinar ‚Üí Random Forest ‚Üí Predicci√≥n
  Sex, Embarked, Pclass
        ‚Üì
  Imputar con moda
        ‚Üì
  One-Hot Encoding
        ‚Üì
        ‚îò
```

---

## Ejemplo 3: Pipeline con Feature Engineering

### Creaci√≥n de nuevas caracter√≠sticas

```python
from sklearn.base import BaseEstimator, TransformerMixin

# Crear transformador personalizado
class FeatureCreator(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X = X.copy()
        # Crear nuevas caracter√≠sticas
        X['FamilySize'] = X['SibSp'] + X['Parch'] + 1
        X['IsAlone'] = (X['FamilySize'] == 1).astype(int)
        X['FarePerPerson'] = X['Fare'] / X['FamilySize']
        return X

# Pipeline con feature engineering
pipeline_with_features = Pipeline([
    ('feature_creator', FeatureCreator()),
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(max_iter=1000))
])

# Entrenar
X = df[['Age', 'Fare', 'SibSp', 'Parch']]
y = df['Survived']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

pipeline_with_features.fit(X_train, y_train)
score = pipeline_with_features.score(X_test, y_test)
print(f'Accuracy con Feature Engineering: {score:.2f}')
```

---

## Ejemplo 4: Pipeline con Validaci√≥n Cruzada

```python
from sklearn.model_selection import cross_val_score, GridSearchCV

# Definir pipeline
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression())
])

# Preparar datos
X = df[['Age', 'Fare', 'SibSp', 'Parch']].copy()
y = df['Survived']

# Validaci√≥n cruzada
scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')
print(f'Accuracy promedio (CV): {scores.mean():.2f} (+/- {scores.std():.2f})')

# B√∫squeda de hiperpar√°metros
param_grid = {
    'imputer__strategy': ['mean', 'median'],
    'classifier__C': [0.1, 1.0, 10.0],
    'classifier__solver': ['liblinear', 'lbfgs']
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X, y)

print(f'Mejores par√°metros: {grid_search.best_params_}')
print(f'Mejor score: {grid_search.best_score_:.2f}')
```

**Ventaja**: El pipeline asegura que cada fold de validaci√≥n cruzada aplique las mismas transformaciones.

---

## Ejemplo 5: Pipeline para Detecci√≥n de Outliers

```python
from sklearn.preprocessing import RobustScaler
from sklearn.covariance import EllipticEnvelope

# Pipeline para detecci√≥n de anomal√≠as
outlier_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', RobustScaler()),  # Robusto a outliers
    ('detector', EllipticEnvelope(contamination=0.1))
])

# Entrenar
X = df[['Age', 'Fare']].copy()
outlier_pipeline.fit(X)

# Detectar outliers
predictions = outlier_pipeline.predict(X)
outliers = predictions == -1

print(f'N√∫mero de outliers detectados: {outliers.sum()}')
print(f'Porcentaje de outliers: {(outliers.sum() / len(X)) * 100:.2f}%')
```

---

## Ejemplo 6: Pipeline Completo para Producci√≥n

```python
import joblib
from datetime import datetime

# Pipeline completo y robusto
production_pipeline = Pipeline([
    ('feature_creator', FeatureCreator()),
    ('preprocessor', ColumnTransformer(
        transformers=[
            ('num', Pipeline([
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', RobustScaler())
            ]), ['Age', 'Fare', 'SibSp', 'Parch', 'FamilySize', 'IsAlone', 'FarePerPerson']),
            ('cat', Pipeline([
                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
                ('onehot', OneHotEncoder(handle_unknown='ignore'))
            ]), ['Sex', 'Embarked'])
        ])),
    ('classifier', RandomForestClassifier(
        n_estimators=200,
        max_depth=10,
        min_samples_split=5,
        random_state=42
    ))
])

# Entrenar
X = df[['Age', 'Fare', 'SibSp', 'Parch', 'Sex', 'Embarked']]
y = df['Survived']

production_pipeline.fit(X, y)

# Guardar el pipeline
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
filename = f'titanic_pipeline_{timestamp}.pkl'
joblib.dump(production_pipeline, filename)
print(f'Pipeline guardado como: {filename}')

# Cargar y usar el pipeline
loaded_pipeline = joblib.load(filename)
new_data = pd.DataFrame({
    'Age': [25],
    'Fare': [30.0],
    'SibSp': [0],
    'Parch': [0],
    'Sex': ['male'],
    'Embarked': ['S']
})

prediction = loaded_pipeline.predict(new_data)
probability = loaded_pipeline.predict_proba(new_data)

print(f'Predicci√≥n: {"Sobrevive" if prediction[0] == 1 else "No sobrevive"}')
print(f'Probabilidad: {probability[0][1]:.2%}')
```

---

## Buenas Pr√°cticas al Usar Pipelines

### ‚úÖ Hacer:

1. **Incluir toda la transformaci√≥n de datos** en el pipeline
2. **Usar nombres descriptivos** para cada paso
3. **Guardar el pipeline completo** para producci√≥n
4. **Validar con datos nuevos** despu√©s de cargar el pipeline
5. **Documentar cada paso** del pipeline

### ‚ùå Evitar:

1. **Transformar datos antes del pipeline** (puede causar fuga de datos)
2. **Usar fit_transform en datos de prueba** (solo transform)
3. **Mezclar diferentes versiones** de pipelines en producci√≥n
4. **Olvidar manejar valores desconocidos** en variables categ√≥ricas

---

## Comparaci√≥n: Con Pipeline vs Sin Pipeline

### ‚ùå Sin Pipeline (Propenso a errores):

```python
# Entrenar
X_train_imputed = imputer.fit_transform(X_train)
X_train_scaled = scaler.fit_transform(X_train_imputed)
model.fit(X_train_scaled, y_train)

# Predecir (¬øusamos fit_transform o transform?)
X_test_imputed = imputer.transform(X_test)  # ‚ö†Ô∏è F√°cil olvidar usar transform
X_test_scaled = scaler.transform(X_test_imputed)
predictions = model.predict(X_test_scaled)
```

### ‚úÖ Con Pipeline (Seguro y limpio):

```python
# Entrenar
pipeline.fit(X_train, y_train)

# Predecir (autom√°ticamente usa transform)
predictions = pipeline.predict(X_test)
```

---

## Ejemplo Avanzado: Pipeline con Selecci√≥n de Caracter√≠sticas

```python
from sklearn.feature_selection import SelectKBest, f_classif

# Pipeline con selecci√≥n autom√°tica de caracter√≠sticas
pipeline_with_selection = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('selector', SelectKBest(f_classif, k=3)),  # Seleccionar las 3 mejores
    ('classifier', LogisticRegression())
])

X = df[['Age', 'Fare', 'SibSp', 'Parch']].copy()
y = df['Survived']

pipeline_with_selection.fit(X, y)

# Ver qu√© caracter√≠sticas se seleccionaron
selected_features = pipeline_with_selection.named_steps['selector'].get_support()
feature_names = X.columns
selected_names = feature_names[selected_features]
print(f'Caracter√≠sticas seleccionadas: {list(selected_names)}')
```

---

## Pipeline para Diferentes Tipos de Problemas

### Clasificaci√≥n Binaria (Sobrevivi√≥ o No):

```python
classification_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())
])
```

### Regresi√≥n (Predecir Edad):

```python
from sklearn.ensemble import RandomForestRegressor

regression_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor())
])
```

### Clustering (Agrupar pasajeros):

```python
from sklearn.cluster import KMeans

clustering_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('clusterer', KMeans(n_clusters=3))
])
```

---

## Resumen

Un **Pipeline** es una herramienta fundamental en Machine Learning que:

- üîÑ **Automatiza** el flujo de trabajo
- üõ°Ô∏è **Previene errores** comunes
- üì¶ **Facilita el despliegue** en producci√≥n
- üî¨ **Mejora la reproducibilidad** de experimentos
- üéØ **Simplifica la validaci√≥n cruzada** y b√∫squeda de hiperpar√°metros

### Estructura b√°sica:

```python
Pipeline([
    ('paso1', Transformador1()),
    ('paso2', Transformador2()),
    ('paso3', Estimador())
])
```

**Recuerda**: Todo lo que se aplica a los datos de entrenamiento debe estar dentro del pipeline para garantizar que se aplique correctamente a los datos de prueba y producci√≥n.

---

## Recursos Adicionales

- [Documentaci√≥n oficial de Scikit-Learn Pipelines](https://scikit-learn.org/stable/modules/compose.html)
- [Ejemplos de ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)
- [Crear transformadores personalizados](https://scikit-learn.org/stable/developers/develop.html)
